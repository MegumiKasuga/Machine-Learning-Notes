CNN(Convolutional Neural Networks，卷积神经网络)是一种新兴的神经网络类型，它具有神经网络模型所具有的多功能性，也能够解决传统神经网络所具有的一些问题。卷积神经网络是当前世界上功能最为强大的机器学习模型。

### 传统神经网络的缺陷
- 贫瘠的缩放特性
	当输入的维度数改变，就需要重新训练一整个网络。
- 容易过拟合
	如果层数太多，那么伴随的参数数量会非常庞大。
- 难以利用局部特征
	会忽视隐藏在高维数据(如图片数据)中的高维特征。
	相邻层之间总是全连接的；而不是聚焦于某些局部区域。

比如：传统神经网络不能判断偏移了的数字。
![[Pasted image 20241230195700.png]]

### CNN的设计目的
- 极大降低过拟合的风险
	先验条件：局部平滑。
- 利用数据中的局部结构
	局部结构比全局特征具有更好的泛化性。
	很多有用的信息都处于局部结构当中。
- 提高了神经网络的鲁棒性
	平移等价：位置的变化会导致位置上输出的变化。
	平移不变性：位置的变化不会改变输出。

### 关于图像数据
图像数据有灰度图和彩色图两种。
**灰度图**：只有一个通道，也就是说它是一个大小为图片的宽乘高的矩阵，其中的值是该像素点的灰度值。
**彩色图**：有三个通道，它是三个大小为图片的宽乘高的矩阵，其中的值是该种颜色的亮度值。
**通道(Channel)**：也称为深度(depth)。
![[Pasted image 20241230203747.png]]

### 卷积(Convolution)
CNN中的卷积是一种聚合操作，通过这种操作，卷积神经网络就可以有效地将矩阵数据中的特征提取出来，这样就可以克服掉传统神经网络中网络无法处理数据的平移旋转等问题的缺陷。
![[卷积.gif]]
如图所示，对原图片(蓝色)使用两种不同的卷积核(红色)进行卷积，得到的结果就是绿色的矩阵。对于每一次计算，就是将窗口所选中的矩阵(原图中蓝色3乘3的框框)和卷积核做内积(就是$x_{1}\cdot c_{1}+x_{2}\cdot c_{2}+\dots+x_{n} \cdot c_{n}$)，最终输出的值填入结果矩阵当中，如图：
![[Pasted image 20241230202828.png]]
卷积过程的参数：
1) 步长(stride)：每次窗口滑动的位置的距离。
2) 卷积核的个数：决定输出的厚度(depth)。
3) 填充值(Zero Padding)：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑动到末尾位置，也可以说使得总长度可以被步长整除。

### 池化(Pooling)
池化是一种减小计算复杂性的手段，它通过减小特征图的大小来减小计算复杂性。类似于卷积，它也是通过使用滑动窗口扫描全图来实现的，扫描时，它会选择窗口内的最大值或该窗口的平均值来作为池化的输出。一般来说，经过池化之后的数据大小可以直接缩小为原图的$\frac{1}{2}$，$\frac{1}{3}$甚至更多，这取决于你使用多大的池化窗口来实现。池化可以**快速提取图片数据中的高阶特征**。
![[Pasted image 20241230205116.png]]
**最大化池化(Max Pooling)**：如下图。最大化池化即选择滑动窗口当中最大的一个值作为池化的输出：
![[Pasted image 20241230205137.png]]
**平均池化(Average Pooling)**：如下图。平均池化即计算滑动窗口当中的平均值作为池化的输出：
![[Pasted image 20241230205325.png]]
#### 池化操作的效果
- 下采样(down-sampling)以剔除多余信息。
	会有信息损失。
- 减少特征通道的空间维度。
	减小特征通道的大小。
- 引入更多非线性变换。
	增强模型抵抗过拟合的能力。
- 引入平移不变性
	即使特征不在它们所处的位置上时，也能注意到它们。

#### 不同池化操作的效果
- 最大化池化
	获得特征选择的效果。
	引入额外非线性映射。
- 平均池化
	保留信息的效果更好。
		能够减少神经元的数量。
	是线性操作。
	全局平均池化(Global Average Pooling, GAP)能够有效抵御高深度CNN的过拟合。

### 激活(Activate)
卷积神经网络中激活层的主要作用是将输出结果作线性映射，激活层由激活函数组成。通过非线性映射，网络将特征映射到高维的非线性区间，以**增强网络的表达能力**。由于传统的$Sigmoid$激活函数在反向求导时的计算量相对较大，且当网络层数逐渐增加时，容易出现梯度消失的情况，而$ReLU$函数计算简单，且具有稀疏激活性，可以一定程度上缓解过拟合现象，因此CNN中一般使用$ReLU$作为激活函数。
**激活层的主要作用：将多个通道变换为所需的深度。如下图(使用一个激活层，压缩为1深度)：**
![[Pasted image 20241230210719.png]]
常用的激活函数(目前最常用的是$ReLU$函数)：
![[Pasted image 20241230210756.png]]
$ReLU$函数的函数形式如下：$$f(x)=\left\{\begin{array} \\
x~~~~~~~~~~if~~x>0 \\
0~~~~~~~~otherwise
\end{array}\right.$$
**使用两个激活层，变换为2深度：**
![[Pasted image 20241230211213.png]]

### CNN的总体结构
##### 卷积块的结构
![[Pasted image 20241230211930.png]]
卷积块指的是**卷积层、激活层、池化层的组合**，如上图。一组卷积层，接一组激活层，接一组池化层，就构成了一个卷积块，卷积块的作用即为提取图像数据中的关键特征。
![[Pasted image 20241230212119.png]]
**卷积神经网络，其实就是一层或数层卷积块连接上一个传统神经网络。**
图像数据经过卷积块处理后，将数据的各维度特征提取出来，之后经过**展开(flatten)**，再输入到**全连接层**(其实就是一个传统神经网络)当中去，最终获得分类结果，如下图。
![[Pasted image 20241230212445.png]]
高深度的CNN通常还会有两个或多个卷积块，如下例：
![[Pasted image 20241230212633.png]]

### $J$代价函数
- CNN的代价函数和传统的神经网络是一样的，因为其分类器仍然是一个传统的神经网络分类器。
- 有些特殊类别的代价函数需要依靠具体处理什么工作来定制。

### 学习算法
- 梯度下降算法，和传统的神经网络相同。

### 深度卷积网络 - 例子
**2012年，AlexNet，开启了深度学习新纪元**
![[Pasted image 20241230213128.png]]
**2014年，GoogleNet/Inceptions**
![[Pasted image 20241230213157.png]]
**2014年，VggNet**
![[Pasted image 20241230213229.png]]
**2015年，ResNet**
![[Pasted image 20241230213249.png]]
**2022年，ConvNeXt**
![[Pasted image 20241230213329.png]]

**目前，Transformers**



