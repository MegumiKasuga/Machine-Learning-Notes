### 模型描述
$m$ - 训练样本行的数量。
$n$ - 每一个样本行的特征数。
$x^{(i)}$ - 训练集中的第i个样本行。
$x^{i}_{j}$ - 训练集中第i个样本行的第j个特征的值。

### $h$函数 - 假设函数
$$h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \dots + \theta_{n}x_{n}$$
为了简化上式，可以添加一个额外的$x_{0}$，并使其总为1。
那么可以简化为：$$h_{\theta}(x) = \sum_{i=0}^n\theta_{i}x_{i}$$
也可以把$x$和$\theta$表示为矢量：$$x=[x_{0}, x_{1},\dots, x_{n}]^T$$$$\theta = [\theta_{0},\theta_{1},\dots, \theta_{n}]^T$$
那么模型可以简化为：$$h_{\theta}(x)=\theta^Tx$$
### $J$函数 - 代价函数
将$x$和$\theta$写成矢量形式，那么多元线性回归函数的代价函数可写为：$$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$
梯度下降算法就可以写成：重复下列计算直至收敛$$\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial \theta_{j}}J(\theta), ~~~~~j=0,\dots,n$$
将$J$函数代入梯度下降算法当中：$$\theta_{j}:=\theta_{j}-\alpha \frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}, ~~~~~j=0,\dots,n$$
#### 加速梯度下降速度的方法
1. 特征缩放
	将多个特征的大小缩放到相近的范围(最简单的，归一化)，那么这个模型的梯度下降速度就可以显著加快。一般来说，缩放到 $[-1, 1]$ 范围就比较好。一般来说缩放的公式如下：$$\frac{x_{1}-\mu_{1}}{s_{1}}\to x_{1}$$
	这里的$\mu$就是这个变量的平均值，$s$则是变量的极差或者标准差。
2. 学习率$\alpha$
	保证梯度下降正常工作。画出迭代数 - $min_{\theta}J(\theta)$的图像，当该图像已经趋于平缓时，就应当选择是否继续迭代；这个函数也可以检查梯度下降是否收敛(这个函数应当类似反比例函数，如果这个函数值随着次数增大上升，则说明梯度下降发散了)。

