无监督学习(Unsupervised Learning)指的是对于某个没有预先给定标签的样本集合$(x_{1},x_{2},\dots,x_{n})$，使得机器学习算法能够寻找出该样本集合的一些结构特征(比如找出这个样本集的类特征或群特征，也就是聚类(Clustering))。

### K-Means算法
K均值算法(K-Means)算法是当今世界上运用最为广泛的一种无监督学习算法。它能够将一个样本集合聚合到多个类别当中。
K-Means算法是一个迭代算法，主要步骤有两步：
- 簇分配。即计算样本到每个簇中心的距离，选择最近的那个簇中心作为该样本点的类。
- 移动聚类中心。即根据簇分配中的类属情况，将聚类中心移动到该类别的样本平均处。

#### 引例
假设现有一个点集$\{x\}$，要将该点集分配到两个簇中(分别以蓝色和红色进行标记)，如下图：
![[Pasted image 20241230190134.png]]
首先选定两个聚类中心，如上图。
接着，遍历样本集合中的每个点，找出其和哪一个样本中心的距离最小，将其标记为对应的颜色，如下图。
![[Pasted image 20241230190304.png]]
计算出各类的样本平均，然后将聚类中心移动到该类的样本中心处，如下图。
![[Pasted image 20241230190456.png]]
把所有点的颜色抹掉，重新进入簇分配步骤，直至簇中心稳定。
![[Pasted image 20241230190546.png]]
![[Pasted image 20241230190601.png]]
最终我们就成功获得了两个簇，它们都是稳定的。

### K-Means算法的设计
输入值: 
- $K$簇个数。
- 训练集$\{x^{(1)},x^{(2)},\dots,x^{(n)}\}$，其中$x^{(i)} \in \mathbb{R}^n$。

工作过程：
	随机初始化$K$个聚类中心$\mu_{1},\mu_{2},\dots \mu_{K} \in \mathbb{R}^n$。
	不断重复：簇分配和移动聚类中心，直至聚类中心不变。
	如果在聚类过程中出现类没有任何点的聚类中心，那么就直接移除这个点(或者重新随机初始化这个聚类中心)

K-Means算法也可以用于区分看起来分类不佳(粘连)的类别，比如下图，若$K=3$，那么K-Means算法就有可能把它们聚类如下三个类：
![[Pasted image 20241230191711.png]]

### 优化目标($J$代价函数)
**符号定义：**
$c^{(i)}$：样本$x^{(i)}$所属的簇的索引。
$\mu_{k}$：类别$k$的聚类中心($\mu_{k} \in \mathbb{R}^n$)。
$\mu_{c^{(i)}}$：样本$x^{(i)}$所属的簇的聚类中心。

代价函数的定义如下，这个函数也可以叫做失真代价函数，或称为K-Means算法的失真：$$J(c^{(1)},\dots,c^{(m)},\mu_{1},\dots,\mu_{k})=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}||^2$$优化目标为：$$min_{(c^{(1)},\dots,c^{(m)},\mu_{1},\dots,\mu_{k})}J(c^{(1)},\dots,c^{(m)},\mu_{1},\dots,\mu_{k})$$即使得**每个点到其聚类中心的距离平方的和最小**，我们的每一次簇分配步骤事实上就在进行这一步的迭代，而移动聚类中心，事实上就是在选择进行下一次迭代所需的$\mu$值。

### 随机初始化
随机初始化K-Means算法是非常重要的步骤。对于一个K-Means算法而言，随机初始化的效果直接关系到计算速度、迭代次数和最终计算的结果质量。
一般来说，我们使用以下的方法进行随机初始化：
1) 首先选定$K<m$，显然聚类个数不应当和样本数相同或比样本数还要大。
2) 随机选择$K$个训练数据。
3) 使得$\mu_{1},\dots,\mu_{k}$分别为这$K$个训练数据。
我们的K Means算法可能会因为初始分类导致陷入局部最优，并不一定能够获得最好的聚类结果，因此我们可以多尝试几次初始化，确保我们能获取到一个最够好的聚类结果。
![[Pasted image 20241230193627.png]]
比如可以进行100次随机初始化，每次聚类完成后都计算出$J$函数，最终取$J$值最小的那个作为最终的聚类结果。

### 选取聚类数量
**肘部法则**
一般来说，我们可以尝试多个不同聚类数量的聚类，比如聚类数量从2开始，到3, 4, 5, ...... 把每个聚类的值画在一个图中，可以得到以下的图像：
![[Pasted image 20241230194109.png]]
这时候，如果拐点足够清晰，那么我们就可以选择这个拐点的位置作为我们的聚类数量。当然了，这个法则并不是一直适用，有时候我们的图像比较平缓(不能判断"肘部"的位置，比如右图)，那么这时候我们就没办法通过肘部法则来判断应该选取几个簇了。

**根据聚类方式的现实意义来选择**
除了肘部法则，我们也可以根据在后续步骤中的现实意义来选择应当使用多少个聚类。比如假设要对T恤衫的大小进行分类，这时候可以从该商品的商业价值进行考虑，来决定应该给它分几类。



