定义：
1. 在没有明确设置的情况下，使得计算机具有学习能力的研究领域。
2. 计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测定在T上的表现因经验E而提高。

### 机器学习的分类
1. **监督学习(Supervised Learning)：即我们教会计算机做某件事情。**
2. **无监督学习(Unsupervised Learning)：计算机自己学会做某件事情。**
3. 强化学习(Reinforcement Learning)
4. 推荐系统(Rencommender Systems)

#### 监督学习(Supervised Learning)
我们给算法一个数据集，其中已经包含了正确答案，算法的目的即给出更多的正确答案。一般来说也被称为
1. **回归问题**(Regression Problem，或连续分类问题)。这种问题的输出是连续的或者输出类数大于2。
2. **分类问题**(Classification Problem二分类问题，分类出0或1；或多分类问题，输出一组0或1)。这种问题的机器输出有且只有0或1(正确与否)两个值。

#### 无监督学习(Unsupervised Learning)
数据集没有预先给定标签，其中没有是否正确的区分，算法的目的是找出这些数据之间的关系(或者确定这个数据集的结构)。一般来说，无监督学习算法可以把数据集分为几个不同的簇(Cluster)，故无监督学习算法也可以被称为**聚类算法**(Clustering)。

## 一元线性回归
### 模型描述
#### 监督学习
$m$ - 训练样本的数量，即训练集中具有的点(样本)的数量。
x's - 输入值/输入特征。
y's - 输出值/输出特征。
(x, y) - 训练样本中的每个点，如$(x^i, y^i)$表示训练集中第$i$个点(样本)。

##### $h$函数 - 假设函数
$h$函数即模型中从输入特征($x$)引导出输出特征($y$)的函数。$$\hat{y}=h_{\theta}(x)$$
假设现有一个假设函数$h$，其形式为：(一般称这样的模型为线性回归模型)$$h_{\theta}(x)=\theta_{0} + \theta_{1}x$$
我们称$\theta_{0}, \theta_{1}, \dots, \theta_{n}$为参数(Parameters)。提高回归精度，即对假设函数$h$进行调参(调整$\theta_{i}$使得模型的拟合效果最好)。

##### $J$函数 - 代价函数
在线性回归问题中，我们要处理一个最小化问题。即整个模型的平均误差取得最小值。每个样本的估计误差：$$h_{\theta}(x)^{(i)} - y^{(i)}$$
**代价函数**(也称平均误差函数)一般规定为**总体方差的一半**，对于上述例子而言即是如此：$$J(\theta_{0}, \theta_{1})=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x)^{(i)} - y^{(i)})^2$$
当总体方差取得最小值时，我们可以称我们的假设函数取得最优。
调参即找到一列$\theta$，使得$J$能够取得最小值。一般来说，对于大部分的回归问题，平均误差代价函数都能很好地发挥其作用。

### 梯度下降法
梯度下降法可以用以自动求出函数的极小值等，在很多领域都具有广泛用途。
接下来使用梯度下降法最小化代价函数$J$。
梯度下降法的过程：
1. 从某组$\theta_{0}, \theta_{1}, \dots, \theta_{n}$开始
2. 不断改变$\theta's$，来减小$J(\theta's)$，直到$J$最终取得最小值。
梯度下降算法：重复$$\theta_{j} := \theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1}, \dots, \theta_{n}), ~~~(for~~j=0, 1, 2, \dots, n)$$
直至该算法收敛。(:=表示赋值，$\alpha$即该梯度下降的学习率(Learning Rate))
**先用旧值算出所有新值，再同时更新新值，不能算出一个更新一个！**

#### 将梯度下降法应用于平均误差代价函数
$$\frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1}, \dots, \theta_{n}) = \frac{\partial}{\partial \theta_{j}}\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})^2$$
对于上述的例子，即有：
$$j=0~~~\frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1}) =\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})$$
$$j=1~~~\frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1}) =\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}$$
对于线性回归函数而言，代价函数总是一个凸函数，因此只有一个全局最优，没有局部最优，因此梯度下降法总是可用的。
**Batch 梯度下降**：每次梯度下降都使用**整个训练集**的数据。
