![[Pasted image 20241228190608.png]]
对于一个神经网络分类器，作出如下定义：
$L$：网络中的总层数(上图神经网络中$L=4$)
$s_{l}$：第$l$层中的神经元的个数(不包括偏置神经元)。
对于二分类(Binary Classification)任务，输出值$y$为0或1
而对于多分类(Multi-class Classification)，假定区分为$K$个类，那么输出值$y$就是一个$K$维矢量(如上图)，同时当然也需要输出层有$K$个神经元(每个神经元输出负责一个二分类任务)。

### $J$代价函数
对于一个逻辑回归分类器：
$$J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\log(h_{\theta}(x^{(i)})-(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))\right] + \frac{\lambda}{2m}\sum_{j=1}^n\theta_{j}^2$$
对于一个Sigmod神经网络：$$J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^m\sum_{k=1}^K y^{(i)}_{k}\log(h_{\Theta}(x^{(i)}))_{k}-(1-y^{(i)}_{k})\log(1-h_{\Theta}(x^{(i)}))_{k}\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^n(\theta_{ji}^l)^2$$就是把每个神经元都视为一个逻辑回归分类器，把它们的损失相加，得到整个神经网络的总损失。

### 反向传播算法
为了进行梯度下降，我们需要得出每一个参数的偏微分项$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$，而反向传播算法正是为了解决这个问题而生。
$\delta_{j}^{(l)}$：第$l$层中神经元$j$的误。假设一个使用$sigmod$激活函数，$L=4$的神经网络，对于输出层$l=4$中的每个神经元，有：$$\delta_{j}^{(4)}=a_{j}^{(4)}-y_{j}$$那么我们可以反向算出对应的前一层的$\delta$值，有：$$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)} \cdot g'(z^{(3)})$$$$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)} \cdot g'(z^{(2)})$$
其中：$$g'(z^{(3)})=a^{(3)}\cdot(1-a^{(3)})$$$$g'(z^{(2)})=a^{(2)}\cdot(1-a^{(2)})$$在忽略正则化项(或认为正则化项$\lambda=0$的情况下)就可以证明：$$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=a_{j}^{(l)}\delta_{i}^{(l+1)}$$最终就可以进行梯度下降的计算：$$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_{j}^{(l)}\delta_{i}^{(l+1)}$$
加上偏置项的形式：$$D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda \Theta_{ij}^{(l)}~~~,~~~if~~j \neq 0$$$$D_{ij}^{(l)}:=\frac{1}{m} \Delta_{ij}^{(l)}~~~,~~~if~~j = 0$$且有：$$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$$
### 随机初始化
绝大部分的神经网络都需要给神经网络的神经元一个初始权重，但是如果我们把权重全部设置成一样的，那么就会造成网络的算法不能正常工作(或具有非常高的冗余，因为算法无法区分不同输入值了)。因此，我们需要生成随机的初始值给每个神经元。即使得$\Theta_{ij}^{(l)}\in[-\epsilon, \epsilon]$。