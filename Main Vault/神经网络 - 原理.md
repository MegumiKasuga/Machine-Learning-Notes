### 非线性假设
传统的拟合分类器(包括线性和非线性分类器)，在处理具有极大量特征的样本(比如图片)时，它的$h$函数就会变得过于庞大，而这样庞大的特征量会使得传统的分类器表现相当差劲。因此，我们迫切需要一种新的分类模型，要求其在基本组成足够简单，且能够很好地处理特征量巨大的样本，这样的模型就是神经网络模型。

### 神经网络
神经网络曾经在80-90年代应用广泛，在2000年代，由于计算机算力限制，神经网络曾经衰落过一段时间。而在近十年来，随着GPU并行计算的兴起，神经网络又成为了机器学习领域的宠儿。
基于人类对大脑神经结构的研究，神经网络表现出了令人惊叹的多模态特征，一个神经网络(经过针对性训练)可以处理非常多种不同的任务，而不是像传统机器学习算法那样只能处理某种单一任务。

### 神经元
神经元是一个多输入的计算单元，它通过突触从其他神经元(或感受器)获取输入信号，经过自身的计算后，形成一个输出值传递给其他神经元(或输出)。如下图：
![[Pasted image 20241228181737.png]]

### 神经网络的结构
如下图(也可以在每层加入一个偏置层$x_{0}$，这个值总为1，所以也可以不画出)
![[Pasted image 20241228182050.png]]
- $a_{i}^{(j)}$称为第$j$层中第$i$个神经元的**激活函数**。
- $\Theta^{(j)}$是控制函数从第$j$层向第$j+1$层映射的**权重矩阵**。
一般来说，Layer1也被称为输入层；Layer2也被称为隐藏层，隐藏层可以有很多个；Layer3被称为输出层。
如果网络的第$j$层有$s_{j}$个神经元，第$j+1$层有$j_{j+1}$个神经元，那么$\Theta^{(j)}$的维度数就是$s_{j+1}\times(s_{j}+1)$。
对于上述神经网络(假如采用$sigmod$激活函数)，那么对于每个神经元($\Theta_{yx}^{(i)}$的上标表示层数，下标表示其在矩阵中的位置)：$$\begin{array}\\
a_{1}^{(2)}=g(\Theta_{10}^{(1)}x_{0}+\Theta_{11}^{(1)}x_{1}+\Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}) \\
a_{2}^{(2)}=g(\Theta_{20}^{(1)}x_{0}+\Theta_{21}^{(1)}x_{1}+\Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3}) \\
a_{3}^{(2)}=g(\Theta_{30}^{(1)}x_{0}+\Theta_{31}^{(1)}x_{1}+\Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3}) \\
h_{\theta}(x)=a_{1}^{(3)}=g(\Theta_{10}^{(2)}a_{0}^{(2)}+\Theta_{11}^{(2)}a_{1}^{(2)}+\Theta_{12}^{(2)}a_{2}^{(2)}+\Theta_{13}^{(2)}a_{3}^{(2)})
\end{array}$$其中$\Theta^{(i)}_{y0}a_{0}^{(i)}$表示的是每一层的偏置($bias$)，这个值是一个定值，用于给下一层的每个神经元都加上固定的偏移量。
**神经网络的架构**：神经网络的架构指的是神经网络的组成结构，其包括输入层、隐藏层和输出层，也指代各层神经元之间的连接方式。

