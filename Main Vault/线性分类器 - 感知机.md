感知机(Perceptron)是一种**二分类的线性分类模型**，属于**监督学习算法**的一种。输入为实例的特征向量，输出为实例的类别(+1类和-1类)。感知机的目的在于：<mark style="background: #FF5582A6;">寻找出将数据空间中的实例划分为两类的一个超平面。</mark>为求得该超平面，感知机使用基于误分类的代价函数，利用梯度下降法对代价函数进行优化。
<mark style="background: #FFB86CA6;">若训练数据集是线性可分的，那么感知机一定可以求出分离超平面。</mark>若数据集是非线性可分的，那么感知机则不能获得分类超平面。

### 感知机的模型结构
假设有一个训练数据集为$D=\{(x_{i},y_{i})\}^m_{i=1}$，其中$x_{i} \in X \subseteq R^n$，$y_{i} \in Y=\{+1,-1\}$，即：$$x_{i}=\left[\begin{array} \\
x_{i 1} \\
x_{i 2} \\
\dots \\
x_{i n}
\end{array}
\right]~~~,~~~y_{i}=+1~~or~~-1$$那么，感知机的模型算法即为：$$f(x)=sign(\theta \cdot x + b)~~~, ~~~\theta=\left[\begin{array} \\
\theta_{1} \\
\theta_{2} \\
\dots \\
\theta_{n}
\end{array}\right]~~~,~~~b \in R$$其中$sign()$为符号函数, $\theta$为参数矢量(或称为权值或权值矢量$\omega$)，$x$为输入的特征矢量, $b$称为偏置(bias)。
![[Pasted image 20241229193116.png]]

### $J$代价函数
感知机使用误分类点到超平面的总距离作为代价函数。
假设超平面为$h=\omega \cdot x + b$，那么样本点$x'$到超平面的距离：$$d=\frac{\omega \cdot x'+b}{||\omega||}$$其中，$||\omega||$为$\omega$的$L_{2}$范数(也就是$\omega$矢量的模)。显然，任意一个样本点到超平面$S$的距离都满足上述公式，即：$$d=\frac{|\omega \cdot x_{i}+b|}{||\omega||}$$
那么，对误分类点集$M$中的所有点，其到超平面$S$的总距离为(这里乘$y_{i}$其实就等于上面的式子中的绝对值，因为对于误分类点来说，$y_{i}$总是-1，而$\omega \cdot x_{i}+b$也总是负值，乘上就是正的了)：$$-\frac{1}{||\omega||}\sum_{x_{i \in M}}y_{i}(\omega \cdot x_{i}+b)$$那么损失函数即为(这里可以无视掉$\frac{1}{||\omega||}$)：$$L(\omega,b)=-\sum_{x_{i} \in M}y_{i}(\omega \cdot x_{i}+b)$$
### 训练过程
1) 选取初值$\omega_{0},~b_{0}$
2) 在训练数据集中选取数据$(x_{i},y_{i})$
3) 判断该数据点是否是当前模型的误分类点，即判断$y_{i}(\omega \cdot x_{i} + b)\leq 0$，若成立那么进行更新：$$\omega :=\omega + \eta y_{i}x_{i}~~~,b:=b+\eta y_{i}$$
4) 跳转到第(2)步，直至训练集中没有误分类点。

### 优缺点
**优点：**
- 简单有效(不涉及超参数，固定学习率 η = 1)。
- 对于线性可分问题，保证在有限步内收敛。 
**缺点：**
- 依赖于初始化 。
- 依赖于数据顺序。 
- 对于非线性可分问题，永不收敛。 
- 不提供概率输出。 
- 不易扩展到多类别分类问题。

